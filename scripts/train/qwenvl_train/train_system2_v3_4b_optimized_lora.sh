#!/bin/bash

# ============================================
# System2 ç¬¬ä¸‰æ¬¡å¾®è°ƒè®­ç»ƒè„šæœ¬ - v3 (4Bæ¨¡å‹ä¼˜åŒ–LoRAç‰ˆ)
# 
# ä¼˜åŒ–ç‚¹ï¼š
# 1. gradient_checkpointing=True (LLM éœ€è¦ä¿ç•™é¿å… OOM)
# 2. dataloader_persistent_workers=True - ä¿æŒ worker è¿›ç¨‹æ´»è·ƒ
# 3. dataloader_pin_memory=True - GPU å†…å­˜å›ºå®šåŠ é€Ÿ
# 4. dataloader_prefetch_factor=2 - é¢„å– 2 ä¸ª batch
# 5. dataloader_num_workers=16 - ä¿æŒä¸å˜ï¼ˆå·²è¯æ˜æœ‰æ•ˆï¼‰
# 
# Vision Tower çš„æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼šè¢«ä»£ç è‡ªåŠ¨å…³é—­
# 
# GPU: H100 x 4 (GPU 0,1,2,3)
# ============================================

set -e

# è®¾ç½®ä½¿ç”¨çš„GPU
export CUDA_VISIBLE_DEVICES=0,1,2,3

# è®¾ç½®PYTORCHæ˜¾å­˜åˆ†é…ä¼˜åŒ–
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# æ¿€æ´»condaç¯å¢ƒ
source /data/houdekai/miniconda3/bin/activate internnav

# è®¾ç½®wandbåœ¨çº¿æ¨¡å¼
export WANDB_MODE=online

# åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
cd /data/houdekai/InternNav_

# åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
MASTER_ADDR=localhost
MASTER_PORT=$((RANDOM % 101 + 20001))
NUM_GPUS=4

# DeepSpeedé…ç½®
deepspeed=scripts/train/qwenvl_train/zero2.json

# ============================================
# æ¨¡å‹é…ç½® - 4B ç‰ˆ
# ============================================
llm=/data/houdekai/models/Qwen3-VL-4B-Instruct

# ============================================
# è®­ç»ƒæ•°æ®é…ç½®
# ============================================
vln_datasets="r2r_125cm_0_30,rxr_125cm_0_30,scalevln_125cm_0_30%50"

# ============================================
# è®­ç»ƒå‚æ•° (å‚è€ƒ v2 è„šæœ¬é…ç½®)
# ============================================
# å…¨å±€ batch size = 4 * 12 * 3 = 144 (ä¸ v2 ä¿æŒä¸€è‡´)
batch_size=12
grad_accum_steps=3

# LoRA é…ç½®ä¿æŒä¸€è‡´
use_lora=True
lora_r=32
lora_alpha=64
lora_dropout=0.05

# å­¦ä¹ ç‡ - å‚è€ƒ v2 çš„ 2e-4 ç»Ÿä¸€å­¦ä¹ ç‡
lr=2e-4
mm_projector_lr=2e-4
vision_tower_lr=2e-4

# åƒç´ èŒƒå›´ - å‚è€ƒ v2 çš„é…ç½®
min_pixels=3136
max_pixels=313600

# å†å²å¸§å’Œé‡‡æ · - å‚è€ƒ v2
num_history=8
sample_step=4

# Epoch æ•°ï¼šç”¨æˆ·æŒ‡å®š 2 epoch
num_epochs=2

# è¾“å‡ºç›®å½•
output_dir="checkpoints/InternVLA-N1-System2-Qwen3-4B-AllLoRA-r32-v3"
run_name="InternVLA_N1_System2_4B_v3_mixed_data"

# ============================================
# åˆ›å»ºè¾“å‡ºç›®å½•
# ============================================
mkdir -p ${output_dir}

# ============================================
# æ‰“å°é…ç½®ä¿¡æ¯
# ============================================
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸš€ System2 v3 è®­ç»ƒå¼€å§‹ (4B æ¨¡å‹ä¼˜åŒ–LoRAç‰ˆ)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“Š æ¨¡å‹é…ç½®:"
echo "   â€¢ åŸºç¡€æ¨¡å‹: Qwen3-VL-4B-Instruct"
echo "   â€¢ è¾“å‡º: InternVLA-N1-System2-Qwen3-4B-AllLoRA-r32-v3"
echo ""
echo "ğŸ“š æ•°æ®é›†:"
echo "   â€¢ R2R (125cm_0_30)"
echo "   â€¢ RxR (125cm_0_30)"
echo "   â€¢ ScaleVLN (125cm_0_30) â­ æ–°å¢å¤§è§„æ¨¡æ•°æ®"
echo ""
echo "âš™ï¸ è®­ç»ƒå‚æ•°:"
echo "   â€¢ Batch Size: ${batch_size} x ${NUM_GPUS} GPUs x ${grad_accum_steps} grad_accum = $(($batch_size * $NUM_GPUS * $grad_accum_steps))"
echo "   â€¢ Learning Rate: ${lr}"
echo "   â€¢ LoRA r: ${lora_r}, alpha: ${lora_alpha}"
echo "   â€¢ Epochs: ${num_epochs}"
echo ""
echo "âš¡ æ€§èƒ½ä¼˜åŒ–:"
echo "   â€¢ gradient_checkpointing: True (LLM ä¿ç•™é¿å… OOM)"
echo "   â€¢ Vision Tower Grad Ckpt: è‡ªåŠ¨å…³é—­ âœ…"
echo "   â€¢ dataloader_num_workers: 16 (ä¿æŒ)"
echo "   â€¢ dataloader_persistent_workers: True âœ…"
echo "   â€¢ dataloader_pin_memory: True âœ…"
echo "   â€¢ dataloader_prefetch_factor: 2 âœ…"
echo ""
echo "ğŸ“ è¾“å‡º: ${output_dir}"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "GPU ä¿¡æ¯:"
nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv | head -10
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# ============================================
# å¯åŠ¨è®­ç»ƒ (Pure LoRA + æ•°æ®åŠ è½½ä¼˜åŒ–)
# ============================================
torchrun --nnodes=1 --nproc_per_node=${NUM_GPUS} \
    --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT} \
    internnav/trainer/internvla_n1_trainer.py \
    --deepspeed ${deepspeed} \
    --model_name_or_path "${llm}" \
    --vln_dataset_use ${vln_datasets} \
    --data_flatten False \
    --tune_mm_vision True \
    --tune_mm_mlp True \
    --tune_mm_llm True \
    --use_lora ${use_lora} \
    --lora_r ${lora_r} \
    --lora_alpha ${lora_alpha} \
    --lora_dropout ${lora_dropout} \
    --bf16 True \
    \
    --num_history ${num_history} \
    --data_augmentation True \
    --resize_h 384 \
    --resize_w 384 \
    --sample_step ${sample_step} \
    --num_future_steps 4 \
    --predict_step_num 32 \
    --pixel_goal_only False \
    --system1 "none" \
    \
    --output_dir ${output_dir} \
    --num_train_epochs ${num_epochs} \
    --per_device_train_batch_size ${batch_size} \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps ${grad_accum_steps} \
    --max_pixels ${max_pixels} \
    --min_pixels ${min_pixels} \
    --eval_strategy "no" \
    --save_strategy "steps" \
    --save_steps 500 \
    --save_total_limit 3 \
    --learning_rate ${lr} \
    --mm_projector_lr ${mm_projector_lr} \
    --vision_tower_lr ${vision_tower_lr} \
    --weight_decay 0.01 \
    --warmup_ratio 0.03 \
    --max_grad_norm 1.0 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --model_max_length 8192 \
    --gradient_checkpointing True \
    --dataloader_num_workers 16 \
    --dataloader_persistent_workers True \
    --dataloader_pin_memory True \
    --dataloader_prefetch_factor 2 \
    --run_name ${run_name} \
    --report_to wandb

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Training v3 (4B LoRA) å®Œæˆ!"
echo ""
echo "ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ${output_dir}"
echo ""
echo "ğŸ” ä¸‹ä¸€æ­¥: è¿è¡Œè¯„ä¼°è„šæœ¬"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# ============================================
# [æ–°å¢] åˆ†å¸ƒå¼è®­ç»ƒé€šä¿¡ä¼˜åŒ– (è§£å†³ GPU å¡é¡¿)
# ============================================
